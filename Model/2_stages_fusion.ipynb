{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1zkMjUgAegY"
      },
      "source": [
        "### GPU info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN0XPaF2AjzK"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tGaekNkUp8_"
      },
      "source": [
        "### Drive連結"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnHuSwLjt8yG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGynxt4w_yNB"
      },
      "source": [
        "### 切換路徑"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFXiA5d01lx6"
      },
      "outputs": [],
      "source": [
        "cd drive/MyDrive/colab_pro_plus/CIKM_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPAbmQ4DANlg"
      },
      "source": [
        "## Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxZpG_QUANlg"
      },
      "source": [
        "### Lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42iXEetaANlg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Util #\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.sparse import linalg\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Layer #\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import numbers\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Model #\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import sys\n",
        "\n",
        "# Trainer #\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# Main #\n",
        "import torch\n",
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m9BJMEvANlh"
      },
      "source": [
        "### Util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3lArIYWANlh"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataLoaderM(object):\n",
        "    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):\n",
        "        \"\"\"\n",
        "        :param xs:\n",
        "        :param ys:\n",
        "        :param batch_size:\n",
        "        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.current_ind = 0\n",
        "\n",
        "        # 將資料長度補齊至batch_size可整除之數量\n",
        "        # 補齊方法: 取原資料最後一個並複製多個來補齊\n",
        "        if pad_with_last_sample:\n",
        "            # 計算需補齊數量\n",
        "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
        "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
        "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
        "\n",
        "            # 將複製後的ele進行concatenate以補齊成可整除batch_size之長度\n",
        "            xs = np.concatenate([xs, x_padding], axis=0)\n",
        "            ys = np.concatenate([ys, y_padding], axis=0)\n",
        "        self.size = len(xs)\n",
        "        self.num_batch = int(self.size // self.batch_size)\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    def shuffle(self):\n",
        "        permutation = np.random.permutation(self.size)\n",
        "        xs, ys = self.xs[permutation], self.ys[permutation]\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    def get_iterator(self):\n",
        "        self.current_ind = 0\n",
        "        def _wrapper():\n",
        "            while self.current_ind < self.num_batch:\n",
        "                start_ind = self.batch_size * self.current_ind\n",
        "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
        "                x_i = self.xs[start_ind: end_ind, ...]\n",
        "                y_i = self.ys[start_ind: end_ind, ...]\n",
        "                # 節省記憶體:\n",
        "                # yield 設計來的目的，就是為了單次輸出內容\n",
        "                # 我們可以把 yield 暫時看成 return，但是這個 return 的功能只有單次\n",
        "                # 而且，一旦我們的程式執行到 yield 後，程式就會把值丟出，並暫時停止\n",
        "                yield (x_i, y_i)\n",
        "                self.current_ind += 1\n",
        "\n",
        "        return _wrapper()\n",
        "\n",
        "class StandardScaler():\n",
        "    \"\"\"\n",
        "    Standard the input\n",
        "    \"\"\"\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "    def transform(self, data):\n",
        "        return (data - self.mean) / self.std\n",
        "    def inverse_transform(self, data):\n",
        "        return (data * self.std) + self.mean\n",
        "\n",
        "def asym_adj(adj):\n",
        "    \"\"\"Asymmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1)).flatten()\n",
        "    d_inv = np.power(rowsum, -1).flatten()\n",
        "    d_inv[np.isinf(d_inv)] = 0.\n",
        "    d_mat= sp.diags(d_inv)\n",
        "    return d_mat.dot(adj).astype(np.float32).todense()\n",
        "\n",
        "\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f)\n",
        "    except UnicodeDecodeError as e:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f, encoding='latin1')\n",
        "    except Exception as e:\n",
        "        print('Unable to load data ', pickle_file, ':', e)\n",
        "        raise\n",
        "    return pickle_data\n",
        "\n",
        "def load_adj(pkl_filename, adjtype):\n",
        "    sensor_ids, sensor_id_to_ind, adj_mx = load_pickle(pkl_filename)\n",
        "\n",
        "    print('# 全部L.A.的sensor ID(sensor_ids):\\n',sensor_ids)\n",
        "    print('# 將sensor ID對應index(sensor_id_to_ind):\\n',sensor_id_to_ind)\n",
        "\n",
        "    if adjtype == \"scalap\":\n",
        "        adj = [calculate_scaled_laplacian(adj_mx)]\n",
        "    elif adjtype == \"normlap\":\n",
        "        adj = [calculate_normalized_laplacian(adj_mx).astype(np.float32).todense()]\n",
        "    elif adjtype == \"symnadj\":\n",
        "        adj = [sym_adj(adj_mx)]\n",
        "    elif adjtype == \"transition\":\n",
        "        adj = [asym_adj(adj_mx)]\n",
        "    elif adjtype == \"doubletransition\":\n",
        "        adj = [asym_adj(adj_mx), asym_adj(np.transpose(adj_mx))]   # asym_adj(adj_mx): forward transition matrix / asym_adj(np.transpose(adj_mx)): backward transition matrix\n",
        "    elif adjtype == \"identity\":\n",
        "        adj = [np.diag(np.ones(adj_mx.shape[0])).astype(np.float32)]\n",
        "    else:\n",
        "        error = 0\n",
        "        assert error, \"adj type not defined\"\n",
        "\n",
        "    print('# Double transition Transition matrix of Eq 4:\\n',adj)\n",
        "    return sensor_ids, sensor_id_to_ind, adj\n",
        "\n",
        "\n",
        "def masked_mse(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /= torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = (preds-labels)**2\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "def masked_rmse(preds, labels, null_val=np.nan):\n",
        "    return torch.sqrt(masked_mse(preds=preds, labels=labels, null_val=null_val))\n",
        "\n",
        "\n",
        "def masked_mae(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /=  torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = torch.abs(preds-labels)\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "def masked_mape(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /=  torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = torch.abs(preds-labels)/labels\n",
        "    #loss = 2.0 * torch.mean(torch.abs(preds - labels) / (torch.abs(preds) + torch.abs(labels)))\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "def masked_smape(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /=  torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    #loss = torch.abs(preds-labels)/labels\n",
        "    loss = 2.0 * (torch.abs(preds - labels) / (torch.abs(preds) + torch.abs(labels)))\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "def metric(pred, real):\n",
        "    mae = masked_mae(pred,real,0.0).item()\n",
        "    mape = masked_mape(pred,real,0.0).item()\n",
        "    rmse = masked_rmse(pred,real,0.0).item()\n",
        "    smape = masked_smape(pred,real,0.0).item()\n",
        "    return mae,mape,rmse,smape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4fdCQ3TANli"
      },
      "source": [
        "### Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8DYHAPCANli"
      },
      "outputs": [],
      "source": [
        "\n",
        "# GWNET\n",
        "class nconv2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(nconv2,self).__init__()\n",
        "\n",
        "    def forward(self,x, A):\n",
        "        # x: torch.Size([64, 32, 207, 12])\n",
        "        # A: torch.Size([207, 207])\n",
        "        x = torch.einsum('ncvl,vw->ncwl',(x,A))\n",
        "        return x.contiguous()\n",
        "\n",
        "\n",
        "class linear(nn.Module):\n",
        "    def __init__(self,c_in,c_out,bias=True):\n",
        "        super(linear,self).__init__()\n",
        "        self.mlp = torch.nn.Conv2d(c_in, c_out, kernel_size=(1, 1), padding=(0,0), stride=(1,1), bias=bias)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "\n",
        "# Process the input with the Adj matrix #\n",
        "class gcn(nn.Module):\n",
        "    def __init__(self,model_type, adj_mx_len,  c_in,c_out,gdep,dropout,alpha):\n",
        "        super(gcn, self).__init__()\n",
        "\n",
        "        self.model_type = model_type\n",
        "        self.adj_mx_len = adj_mx_len\n",
        "\n",
        "        # 這裡要實驗看看\n",
        "        if self.model_type == \"GWNET\":\n",
        "            self.nconv = nconv2()\n",
        "\n",
        "        c_in = (gdep*adj_mx_len+1)*c_in\n",
        "        self.mlp = linear(c_in,c_out)\n",
        "        self.gdep = gdep\n",
        "        self.dropout = dropout\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self,x,adj):\n",
        "\n",
        "\n",
        "        h = x\n",
        "        out = [h]\n",
        "\n",
        "        if self.model_type == \"GWNET\":\n",
        "            # support: 儲存adj matrix/transpose adj matrix/adaptive matrix(optional)\n",
        "            for a in adj:\n",
        "                # 執行diffision convl 第1層\n",
        "                x1 = self.nconv(x,a)                                  # 將各sensor的值透過adj matrix進行聚合\n",
        "                out.append(x1)\n",
        "\n",
        "                # 執行diffusion convl 第k層 (1<k<=2)\n",
        "                # Paper的diffusion step= 2\n",
        "                # k=0: P*a=x1\n",
        "                # k=1: P*(x1)=P*(P*a)=P^2*a\n",
        "                for k in range(2, self.gdep + 1):\n",
        "                    x2 = self.nconv(x1,a)\n",
        "                    out.append(x2)\n",
        "                    x1 = x2\n",
        "\n",
        "        # 問題: 原論文是將每個H經過MLP後, 最後進行加總\n",
        "        # 可能因為過多MLP層, 故作者將H先串接, 最後將dim=1展開, 再經過MLP\n",
        "        # [1+depth layer] -- cat -> ([64, 96, 207, 13]) -- MLP-> ([64, 32, 207, 13])\n",
        "        ho = torch.cat(out,dim=1)   # torch.Size([64, 96, 207, ?])\n",
        "        ho = self.mlp(ho)\n",
        "\n",
        "        # Paper step: information propagation step -- END #\n",
        "        return ho\n",
        "\n",
        "\n",
        "# dilated_inception: 有[2,3,6,7]\b4個平行層 => 分別處理後再串接 #\n",
        "class dilated_inception(nn.Module):\n",
        "    def __init__(self, kernel_set, cin, cout, dilation_factor=2):\n",
        "        super(dilated_inception, self).__init__()\n",
        "        self.tconv = nn.ModuleList()\n",
        "        self.kernel_set = kernel_set\n",
        "        cout = int(cout/len(self.kernel_set))    # 32/4 = 8\n",
        "\n",
        "        # (1x2) & (1x3) & (1x6) & (1x7)\n",
        "        for kern in self.kernel_set:\n",
        "            self.tconv.append(nn.Conv2d(cin,cout,(1,kern),dilation=(1,dilation_factor)))\n",
        "\n",
        "    def forward(self,input):\n",
        "        if args.log_print:\n",
        "            print(\"# dilated_inception input\", input.shape)\n",
        "\n",
        "        x = []\n",
        "\n",
        "        # 每層拿\"原\"input, 而非處理後的input\n",
        "        # ex:\n",
        "        # kernel_set: 2 -> torch.Size([64, 8, 207, 18])\n",
        "        # kernel_set: 3 -> torch.Size([64, 8, 207, 17])\n",
        "        # kernel_set: 6 -> torch.Size([64, 8, 207, 14])\n",
        "        # kernel_set: 7 -> torch.Size([64, 8, 207, 13])\n",
        "        for i in range(len(self.kernel_set)):\n",
        "\n",
        "            x.append(self.tconv[i](input))\n",
        "\n",
        "            if args.log_print:\n",
        "              print('# kernel_set:', self.kernel_set[i])\n",
        "              print('# self.tconv[i](input):', self.tconv[i](input).shape)\n",
        "\n",
        "        # 依照最後一層的feature dim(-x[-1].size(3)), 縮減各層的dim\n",
        "        # 各層feature只取: [..., -x[-1].size(3): ]\n",
        "        for i in range(len(self.kernel_set)):\n",
        "\n",
        "            if args.log_print:\n",
        "              print('# kernel_set:', self.kernel_set[i])\n",
        "              print('# x[i].shape', x[i].shape)\n",
        "              print('# -x[-1].size(3)', -x[-1].size(3))\n",
        "\n",
        "            x[i] = x[i][...,-x[-1].size(3):]\n",
        "            if args.log_print:\n",
        "              print('# modeified x[i].shape', x[i].shape)\n",
        "\n",
        "        # 8x4 kernel set => (64, 32, 207, 13)\n",
        "        x = torch.cat(x,dim=1)\n",
        "        if args.log_print:\n",
        "          print(\"# final x\", x.shape)\n",
        "        #sys.exit()\n",
        "        return x\n",
        "\n",
        "# Paper's graph adjacency matrix\n",
        "class graph_adaptive(nn.Module):\n",
        "    # k: subgraph size\n",
        "    def __init__(self, model_type, nnodes, k, dim, device, alpha=3, static_feat=None):\n",
        "        super(graph_adaptive, self).__init__()\n",
        "\n",
        "        self.model_type = model_type\n",
        "        self.nnodes = nnodes\n",
        "        '''\n",
        "        if static_feat is not None:\n",
        "            xd = static_feat.shape[1]\n",
        "            self.lin1 = nn.Linear(xd, dim)\n",
        "            self.lin2 = nn.Linear(xd, dim)\n",
        "        else:\n",
        "        '''\n",
        "\n",
        "        if self.model_type == \"GWNET\":\n",
        "            # Paper: \"..initialize node embeddings by a uniform distribution with a size of 10.\"\n",
        "            self.nodevec1 = nn.Parameter(torch.randn(self.nnodes, 10).to(device), requires_grad=True).to(device)\n",
        "            self.nodevec2 = nn.Parameter(torch.randn(10, self.nnodes).to(device), requires_grad=True).to(device)\n",
        "\n",
        "        self.device = device\n",
        "        self.k = k\n",
        "        self.dim = dim\n",
        "        self.alpha = alpha\n",
        "        self.static_feat = static_feat\n",
        "\n",
        "    def forward(self, idx=None):   # idx: 已打亂順序的index\n",
        "        if self.model_type == \"GWNET\":\n",
        "            adj = F.softmax(F.relu(torch.mm(self.nodevec1, self.nodevec2)), dim=1)  # SoftMax(ReLU(E1*E2^T))\n",
        "\n",
        "        return adj\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    __constants__ = ['normalized_shape', 'weight', 'bias', 'eps', 'elementwise_affine']\n",
        "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        self.normalized_shape = tuple(normalized_shape)\n",
        "        self.eps = eps\n",
        "        self.elementwise_affine = elementwise_affine\n",
        "        if self.elementwise_affine:\n",
        "            self.weight = nn.Parameter(torch.Tensor(*normalized_shape))\n",
        "            self.bias = nn.Parameter(torch.Tensor(*normalized_shape))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.elementwise_affine:\n",
        "            init.ones_(self.weight)\n",
        "            init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input, idx):\n",
        "        if self.elementwise_affine:\n",
        "            return F.layer_norm(input, tuple(input.shape[1:]), self.weight[:,idx,:], self.bias[:,idx,:], self.eps)\n",
        "        else:\n",
        "            return F.layer_norm(input, tuple(input.shape[1:]), self.weight, self.bias, self.eps)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '{normalized_shape}, eps={eps}, ' \\\n",
        "            'elementwise_affine={elementwise_affine}'.format(**self.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCXijts6VgTW"
      },
      "source": [
        "### Fusion Model (MGAT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWBPDdtjVgTX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MGAT_base(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, bias=True):\n",
        "        super(MGAT_base, self).__init__()\n",
        "\n",
        "        print('MGAT_base', n_heads, in_channel, num_nodes, dropout)\n",
        "        self.n_head = n_heads\n",
        "        self.f_in = num_nodes\n",
        "        self.a_src = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "        self.a_dst = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_nodes))\n",
        "            nn.init.constant_(self.bias, 0)\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.a_src, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a_dst, gain=1.414)\n",
        "\n",
        "    def forward(self, h):\n",
        "        bs, ch, n, dim = h.size()\n",
        "        h_prime = h\n",
        "        attn_src = torch.matmul(h, self.a_src)\n",
        "        attn_dst = torch.matmul(h, self.a_dst)\n",
        "        attn = attn_src.expand(-1, -1, -1, n) + attn_dst.expand(-1, -1, -1, n).permute(\n",
        "            0, 1, 3, 2\n",
        "        )\n",
        "        attn = self.leaky_relu(attn)\n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.matmul(attn, h_prime)\n",
        "        return output + self.bias, attn\n",
        "\n",
        "class MGAT(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, alpha):\n",
        "        super(MGAT, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.layer = MGAT_base(\n",
        "                    n_heads, in_channel, num_nodes, dropout\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs,ch,n,dim = x.size()\n",
        "        x, attn = self.layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MGAT_module(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, mlp, mlp2, dropout, alpha):\n",
        "        super(MGAT_module, self).__init__()\n",
        "        print('MGAT_module', n_heads, in_channel, num_nodes, dropout, alpha)\n",
        "        self.net = MGAT(n_heads, in_channel, num_nodes, dropout, alpha)\n",
        "\n",
        "        self.mlp_convs = nn.ModuleList()\n",
        "        self.mlp_bns = nn.ModuleList()\n",
        "        last_channel = 32\n",
        "        for out_channel in mlp:\n",
        "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "            last_channel = out_channel\n",
        "\n",
        "        self.mlp_convs2 = nn.ModuleList()\n",
        "        self.mlp_bns2 = nn.ModuleList()\n",
        "        last_channel = n_heads\n",
        "        for out_channel in mlp2:\n",
        "            self.mlp_convs2.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "            last_channel = out_channel\n",
        "\n",
        "        self.lay_norm2 = nn.LayerNorm([32 ,7, num_nodes])\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.dropout2 = nn.Dropout(0.6)\n",
        "    def forward(self,x, x_2):\n",
        "        bs, ch, n, dim = x.size()\n",
        "\n",
        "        x_all = []\n",
        "        x_1_all = []\n",
        "        x_2_all =[]\n",
        "        x_3_all = []\n",
        "        x_4_all = []\n",
        "\n",
        "        for idx in range(n):\n",
        "\n",
        "            x_input = [x[:,:,idx].unsqueeze(2), x_2]\n",
        "\n",
        "            x_input = torch.cat(x_input, dim=2) # 64,32,7,19\n",
        "\n",
        "\n",
        "            x_input_cpy = x_input\n",
        "\n",
        "            for i, conv in enumerate(self.mlp_convs):\n",
        "              x_input = F.relu((conv(x_input)))\n",
        "\n",
        "            x_input_cpy2 = x_input\n",
        "            x_input = self.net(x_input)\n",
        "            x_input = x_input_cpy2+ self.dropout1(x_input)\n",
        "\n",
        "            for i, conv in enumerate(self.mlp_convs2):\n",
        "              x_input = F.relu((conv(x_input)))\n",
        "\n",
        "            x_input = x_input_cpy+ self.dropout2(x_input)\n",
        "\n",
        "            #x_input = self.lay_norm2(x_input)\n",
        "\n",
        "            x_all.append(x_input[:,:,0].unsqueeze(2))\n",
        "\n",
        "\n",
        "        x_tmp = torch.cat(x_all, dim=2)  # (64,32,48,19)\n",
        "\n",
        "        return x_tmp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jenRSFIeANli"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEryDF5qANli"
      },
      "outputs": [],
      "source": [
        "class gginet(nn.Module):\n",
        "    def __init__(self, model_type, gcn_true, buildA_true, gcn_depth, num_nodes, device, predefined_A=None,kernel_set=None, static_feat=None, dropout=0.3, subgraph_size=20, node_dim=40, dilation_exponential=1, conv_channels=32, residual_channels=32, skip_channels=64, end_channels=128, seq_length=12, in_dim=2, out_dim=12, layers=3, propalpha=0.05, tanhalpha=3, layer_norm_affline=True, fusion=None):\n",
        "        super(gginet, self).__init__()\n",
        "\n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.gcn_true = gcn_true\n",
        "        self.buildA_true = buildA_true\n",
        "        self.num_nodes = num_nodes\n",
        "        self.dropout = dropout\n",
        "        self.predefined_A = predefined_A\n",
        "        self.layers = layers\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        self.filter_convs = nn.ModuleList()\n",
        "        self.gate_convs = nn.ModuleList()\n",
        "        self.residual_convs = nn.ModuleList()\n",
        "        self.skip_convs = nn.ModuleList()\n",
        "        self.gconv1 = nn.ModuleList()\n",
        "        self.gconv2 = nn.ModuleList()\n",
        "        self.norm = nn.ModuleList()\n",
        "        self.start_conv = nn.Conv2d(in_channels=in_dim,\n",
        "                                    out_channels=residual_channels,\n",
        "                                    kernel_size=(1, 1))\n",
        "\n",
        "\n",
        "        if fusion != None:  #********#\n",
        "\n",
        "          self.fusion_list = nn.ModuleList()\n",
        "          in_channel = 32\n",
        "          n_heads = 8\n",
        "          dropout = 0\n",
        "          alpha = 0.2\n",
        "          t_len = 13\n",
        "          self.fusion_list.append(\n",
        "              MGAT_module(\n",
        "                n_heads=n_heads, in_channel= in_channel, num_nodes=t_len, mlp=[n_heads],mlp2=[32], dropout=dropout, alpha=alpha\n",
        "              )\n",
        "          )\n",
        "\n",
        "\n",
        "        # 一整個gtnet只會有一組node embedding E1,E2\n",
        "        self.adaptive_mx = graph_adaptive(self.model_type, num_nodes, subgraph_size, node_dim, device, alpha=tanhalpha, static_feat=static_feat)\n",
        "\n",
        "\n",
        "        if self.model_type == \"GWNET\":\n",
        "            kernel_size = 2\n",
        "            self.receptive_field = int((layers/2)* ( (kernel_size-1)+ kernel_size ) + 1)    # kernel: 1,2,1,2...\n",
        "\n",
        "        print(\"# Model Type\", self.model_type)\n",
        "        print(\"# receptive_field\", self.receptive_field)\n",
        "        i=0\n",
        "        if dilation_exponential>1:\n",
        "            rf_size_i = int(1 + i*(kernel_size-1)*(dilation_exponential**layers-1)/(dilation_exponential-1))\n",
        "        else:\n",
        "            rf_size_i = i*layers*(kernel_size-1)+1\n",
        "        new_dilation = 1\n",
        "        for j in range(1,layers+1):\n",
        "\n",
        "            if self.model_type == \"GWNET\":\n",
        "                if j % 2 == 1:\n",
        "                    new_dilation = 1\n",
        "                elif j % 2 == 0:\n",
        "                    new_dilation = 2\n",
        "\n",
        "            # residual_channels: 32, conv_channels: 32 , new_dilation: 1\n",
        "            self.filter_convs.append(dilated_inception(kernel_set, residual_channels, conv_channels, dilation_factor=new_dilation))\n",
        "            self.gate_convs.append(dilated_inception(kernel_set, residual_channels, conv_channels, dilation_factor=new_dilation))\n",
        "\n",
        "\n",
        "            # 1x1 convolution for skip connection\n",
        "            #(0): Conv1d(32, 256, kernel_size=(1, 1), stride=(1,))\n",
        "            #(1): Conv1d(32, 256, kernel_size=(1, 1), stride=(1,))\n",
        "            #(2): Conv1d(32, 256, kernel_size=(1, 1), stride=(1,))\n",
        "            #...\n",
        "            if self.model_type == \"GWNET\" :\n",
        "                self.skip_convs.append(nn.Conv2d(in_channels=conv_channels,\n",
        "                                        out_channels=skip_channels,\n",
        "                                        kernel_size=(1, 1)))\n",
        "\n",
        "            #####   GCN   ##### START\n",
        "            if self.gcn_true:\n",
        "                if self.model_type == \"GWNET\":\n",
        "                    if self.buildA_true:    # 有加入adaptive matrix\n",
        "                        adj_mx_len = 3\n",
        "                    else:\n",
        "                        adj_mx_len = 2\n",
        "                    self.gconv1.append(gcn(self.model_type, adj_mx_len, conv_channels, residual_channels, gcn_depth, dropout, propalpha))\n",
        "\n",
        "\n",
        "            else:\n",
        "                self.residual_convs.append(nn.Conv2d(in_channels=conv_channels,\n",
        "                                                out_channels=residual_channels,\n",
        "                                              kernel_size=(1, 1)))\n",
        "\n",
        "            #####   GCN   ##### END\n",
        "\n",
        "            #####   Normalization   ##### START\n",
        "            if self.model_type == \"GWNET\":\n",
        "                self.norm.append(nn.BatchNorm2d(residual_channels))\n",
        "            #####   Normalization   ##### END\n",
        "\n",
        "            new_dilation *= dilation_exponential\n",
        "\n",
        "\n",
        "\n",
        "        self.end_conv_1 = nn.Conv2d(in_channels=skip_channels,\n",
        "                                             out_channels=end_channels,\n",
        "                                             kernel_size=(1,1),\n",
        "                                             bias=True)\n",
        "        self.end_conv_2 = nn.Conv2d(in_channels=end_channels,\n",
        "                                             out_channels=out_dim,\n",
        "                                             kernel_size=(1,1),\n",
        "                                             bias=True)\n",
        "\n",
        "\n",
        "\n",
        "        self.idx = torch.arange(self.num_nodes).to(device)\n",
        "\n",
        "\n",
        "    def forward(self, input, idx=None, input_2=None):\n",
        "\n",
        "        seq_len = input.size(3)\n",
        "        assert seq_len==self.seq_length, 'input sequence length not equal to preset sequence length'\n",
        "\n",
        "        # Step0: 檢查receptive_field, 不足則padding0\n",
        "        if self.seq_length<self.receptive_field:\n",
        "            input = nn.functional.pad(input,(self.receptive_field-self.seq_length,0,0,0))\n",
        "\n",
        "            if input_2 != None:   #********#\n",
        "              input_2 = nn.functional.pad(input_2,(self.receptive_field-self.seq_length,0,0,0))\n",
        "\n",
        "        # Step0: 利用node idx建立embedding:E1, E2\n",
        "        # 建立Sparse的Adaptive matrix\n",
        "        if self.gcn_true:\n",
        "\n",
        "            # Use adaptive matrix\n",
        "            if self.buildA_true:\n",
        "                if self.model_type == \"GWNET\":\n",
        "                    adp = self.adaptive_mx()\n",
        "                    adp = self.predefined_A + [adp]\n",
        "\n",
        "\n",
        "            else:\n",
        "                adp = self.predefined_A\n",
        "\n",
        "\n",
        "        # Step1: turn([64, 2, 207, 19]) to ([64, 32, 207, 19])\n",
        "        x = self.start_conv(input)      # gct\n",
        "\n",
        "        if self.model_type == \"GWNET\":\n",
        "            skip = 0\n",
        "\n",
        "        if input_2 != None:   #********#\n",
        "            x2 = self.start_conv(input_2)   # cctv\n",
        "            x = self.fusion_list[0](x,x2)\n",
        "\n",
        "\n",
        "        # Layers : 3層 : 19->13->7->1 (取決於TCN取的維度)\n",
        "        for i in range(self.layers):\n",
        "\n",
        "            # Step2: Temporal Model --START #\n",
        "            # 為上一層輸出, ex:  [64, 32, 207, 19] -> [64, 32, 207, 13] -> [64, 32, 207, 7]-> [64, 32, 207, 1]\n",
        "            residual = x\n",
        "\n",
        "            # Tanh\n",
        "            filter = self.filter_convs[i](x)\n",
        "            filter = torch.tanh(filter)\n",
        "\n",
        "            # Sigmoid\n",
        "            gate = self.gate_convs[i](x)\n",
        "            gate = torch.sigmoid(gate)\n",
        "\n",
        "            # Fusion\n",
        "            x = filter * gate\n",
        "\n",
        "            # Step2: Temporal Model --END #\n",
        "\n",
        "            # Step3: Skip after TCN --START #\n",
        "            s = x\n",
        "            '''\n",
        "            # skip_convs #\n",
        "            (0): Conv2d(32, 64, kernel_size=(1, 13), stride=(1, 1))\n",
        "            (1): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))\n",
        "            (2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "            '''\n",
        "            # fusion output:([64, 32, 207, 13])\n",
        "            # skip_convsL 0:([64, 64, 207, 1])\n",
        "            s = self.skip_convs[i](s)\n",
        "\n",
        "            if self.model_type == \"GWNET\":\n",
        "                # 讓上一個skip配合目前的skip\n",
        "                try:\n",
        "                    skip = skip[:, :, :,  -s.size(3):]\n",
        "                except:\n",
        "                    skip = 0\n",
        "\n",
        "            skip = s + skip\n",
        "\n",
        "            # Step3: Skip after TCN --END #\n",
        "\n",
        "\n",
        "            # Step4: GCN --START #\n",
        "            if self.gcn_true:\n",
        "                if self.model_type == \"GWNET\":\n",
        "                    x = self.gconv1[i](x, adp)\n",
        "\n",
        "            else:\n",
        "                x = self.residual_convs[i](x)\n",
        "\n",
        "            # x 經過dilated處理後, 會減少feature維度, ex: 19->13->7->1\n",
        "            # 而residual為上一層輸出, 維度為: 19, 13 ...\n",
        "            # 所以需要配合x進行維度調整: [:, :, :, -x.size(3):], 然後進行elemenet-wise相加\n",
        "            x = x + residual[:, :, :, -x.size(3):]\n",
        "\n",
        "\n",
        "            if self.model_type == \"GWNET\":\n",
        "                x = self.norm[i](x)\n",
        "\n",
        "            # Step4: GCN --END #\n",
        "\n",
        "\n",
        "        x = F.relu(skip)\n",
        "        x = F.relu(self.end_conv_1(x))\n",
        "        x = self.end_conv_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbowdbREANli"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTga9hrXANli"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model,model_2, model_3, lrate, wdecay, clip, step_size, seq_out_len, scaler, scaler_2, device, cl=True):\n",
        "\n",
        "        # GCT\n",
        "        self.scaler = scaler\n",
        "        self.model = model\n",
        "        self.model.to(device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
        "\n",
        "        # CCTV\n",
        "        self.scaler_2 = scaler_2\n",
        "        self.model_2 = model_2\n",
        "        self.model_2.to(device)\n",
        "        self.optimizer_2 = optim.Adam(self.model_2.parameters(), lr=lrate, weight_decay=wdecay)\n",
        "\n",
        "        # Fusion\n",
        "        self.model_3 = model_3\n",
        "        self.model_3.to(device)\n",
        "        self.optimizer_3 = optim.Adam(self.model_3.parameters(), lr=lrate, weight_decay=wdecay)\n",
        "\n",
        "        self.loss = masked_mae\n",
        "        self.clip = clip\n",
        "        self.step = step_size\n",
        "        self.iter = 1\n",
        "        self.task_level = 1\n",
        "        self.seq_out_len = seq_out_len\n",
        "        self.cl = cl\n",
        "\n",
        "    # input: GCT, input_2: CCTV, real_val: gct\n",
        "    def train(self, input,input_2, real_val, idx=None):\n",
        "        self.model.eval()\n",
        "        self.model_2.eval()\n",
        "\n",
        "        output = self.model(input)\n",
        "        output = output.transpose(1,3)\n",
        "        output_2 = self.model_2(input_2)\n",
        "        output_2 = output_2.transpose(1,3)\n",
        "\n",
        "        output = torch.cat([output,input[:,1].unsqueeze(1)],dim=1)\n",
        "        output_2 = torch.cat([output_2,input_2[:,1].unsqueeze(1)],dim=1)\n",
        "\n",
        "        #--------------------------------------------#\n",
        "        self.model_3.train()  #*******#\n",
        "        self.optimizer_3.zero_grad()  #*******#\n",
        "        output = self.model_3(output, idx=idx, input_2=output_2) #*******#\n",
        "        output = output.transpose(1,3)\n",
        "        real = torch.unsqueeze(real_val,dim=1)\n",
        "\n",
        "        predict = self.scaler.inverse_transform(output)\n",
        "\n",
        "        if self.iter%self.step==0 and self.task_level<=self.seq_out_len:\n",
        "            self.task_level +=1\n",
        "            print(\"### cl learning\\n iter\",self.iter,\"\\niter%step\",self.iter%self.step,\"\\ntask_level\",self.task_level)\n",
        "            print(\"# predict len:\", len(predict[:, :, :, :self.task_level]))\n",
        "\n",
        "        if self.cl:\n",
        "            loss = masked_mae(predict[:, :, :, :self.task_level], real[:, :, :, :self.task_level], 0.0)\n",
        "        else:\n",
        "            loss = masked_mae(predict, real, 0.0)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        if self.clip is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(self.model_3.parameters(), self.clip)  #*******#\n",
        "\n",
        "        self.optimizer_3.step() #*******#\n",
        "\n",
        "\n",
        "        mae = masked_mae(predict,real,0.0).item()\n",
        "        mape = masked_mape(predict,real,0.0).item()\n",
        "        rmse = masked_rmse(predict,real,0.0).item()\n",
        "        smape = masked_smape(predict,real,0.0).item()\n",
        "        self.iter += 1\n",
        "        return mae,mape,rmse,smape\n",
        "\n",
        "    def eval(self, model_type, input, real_val, input_2=None):\n",
        "\n",
        "        if model_type == 'gct':\n",
        "          self.model.eval()\n",
        "          output = self.model(input)\n",
        "          output = output.transpose(1,3)\n",
        "\n",
        "          predict = self.scaler.inverse_transform(output)\n",
        "\n",
        "        elif model_type == 'cctv':\n",
        "          self.model_2.eval()\n",
        "          output = self.model_2(input)\n",
        "          output = output.transpose(1,3)\n",
        "\n",
        "          predict = self.scaler_2.inverse_transform(output)\n",
        "\n",
        "        elif model_type == 'fusion':\n",
        "          self.model.eval()\n",
        "          self.model_2.eval()\n",
        "          self.model_3.eval()\n",
        "\n",
        "          output = self.model(input)\n",
        "          output = output.transpose(1,3)\n",
        "          output_2 = self.model_2(input_2)\n",
        "          output_2 = output_2.transpose(1,3)\n",
        "\n",
        "          output = torch.cat([output,input[:,1].unsqueeze(1)],dim=1)\n",
        "          output_2 = torch.cat([output_2,input_2[:,1].unsqueeze(1)],dim=1)\n",
        "\n",
        "          #print('output', output[0,0,0])\n",
        "          #print('output_2', output_2[0,0,0])\n",
        "          output = self.model_3(output, idx=None, input_2=output_2)\n",
        "          output = output.transpose(1,3)\n",
        "\n",
        "          predict = self.scaler.inverse_transform(output)\n",
        "\n",
        "        real = torch.unsqueeze(real_val,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "        loss = self.loss(predict, real, 0.0)\n",
        "        mape = masked_mape(predict,real,0.0).item()\n",
        "        rmse = masked_rmse(predict,real,0.0).item()\n",
        "        smape = masked_smape(predict,real,0.0).item()\n",
        "        return loss.item(),mape,rmse,smape\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrfP4hQAI8xl"
      },
      "source": [
        "### Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK8_9i-OI8xl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def str_to_bool(value):\n",
        "    if isinstance(value, bool):\n",
        "        return value\n",
        "    if value.lower() in {'false', 'f', '0', 'no', 'n'}:\n",
        "        return False\n",
        "    elif value.lower() in {'true', 't', '1', 'yes', 'y'}:\n",
        "        return True\n",
        "    raise ValueError(f'{value} is not a valid boolean value')\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--device',type=str,default='cuda',help='')\n",
        "#parser.add_argument('--data',type=str,default='../data/cellular-traffic-Hsinchu',help='data path')\n",
        "#parser.add_argument('--adj_data',type=str,default='../data/cellular-traffic-Hsinchu/adj_mat_self.pkl',help='adj data path')\n",
        "parser.add_argument('--adjtype',type=str,default='doubletransition',help='adj type')\n",
        "\n",
        "\n",
        "parser.add_argument('--gcn_true', type=str_to_bool, default=True, help='whether to add graph convolution layer')\n",
        "parser.add_argument('--buildA_true', type=str_to_bool, default=True,help='whether to construct adaptive adjacency matrix')\n",
        "#parser.add_argument('--load_static_feature', type=str_to_bool, default=False,help='whether to load static feature')\n",
        "parser.add_argument('--cl', type=str_to_bool, default=True,help='whether to do curriculum learning')\n",
        "\n",
        "parser.add_argument('--gcn_depth',type=int,default=2,help='graph convolution depth')\n",
        "#parser.add_argument('--num_nodes',type=int,default=6,help='number of nodes/variables')\n",
        "parser.add_argument('--dropout',type=float,default=0.3,help='dropout rate')\n",
        "\n",
        "parser.add_argument('--node_dim',type=int,default=40,help='dim of nodes')\n",
        "parser.add_argument('--dilation_exponential',type=int,default=1,help='dilation exponential')\n",
        "\n",
        "parser.add_argument('--conv_channels',type=int,default=32,help='convolution channels')\n",
        "parser.add_argument('--residual_channels',type=int,default=32,help='residual channels')\n",
        "#parser.add_argument('--skip_channels',type=int,default=64,help='skip channels')\n",
        "#parser.add_argument('--end_channels',type=int,default=128,help='end channels')\n",
        "\n",
        "parser.add_argument('--in_dim',type=int,default=2,help='inputs dimension')\n",
        "parser.add_argument('--seq_in_len',type=int,default=12,help='input sequence length')\n",
        "parser.add_argument('--seq_out_len',type=int,default=12,help='output sequence length')\n",
        "\n",
        "\n",
        "parser.add_argument('--batch_size',type=int,default=64,help='batch size')\n",
        "#parser.add_argument('--learning_rate',type=float,default=0.001,help='learning rate')\n",
        "#parser.add_argument('--weight_decay',type=float,default=0.0001,help='weight decay rate')\n",
        "parser.add_argument('--clip',type=int,default=5,help='clip')\n",
        "#parser.add_argument('--step_size1',type=int,default=2500,help='step_size')\n",
        "#parser.add_argument('--step_size2',type=int,default=100,help='step_size')\n",
        "\n",
        "parser.add_argument('--propalpha',type=float,default=0.05,help='prop alpha')\n",
        "parser.add_argument('--tanhalpha',type=float,default=3,help='adj alpha')\n",
        "\n",
        "#parser.add_argument('--num_split',type=int,default=1,help='number of splits for graphs')\n",
        "\n",
        "\n",
        "##### GWNET's diff parameter #####\n",
        "parser.add_argument('--model_type',type=str,default='GWNET',help='model type')\n",
        "parser.add_argument('--skip_channels',type=int,default=256,help='skip channels')\n",
        "parser.add_argument('--end_channels',type=int,default=512,help='end channels')\n",
        "parser.add_argument('--layers',type=int,default=8,help='number of layers')\n",
        "parser.add_argument('--kernel_set',default=[2], type=int, nargs='+')\n",
        "\n",
        "parser.add_argument('--print_every',type=int,default=50,help='')\n",
        "parser.add_argument('--seed',type=int,default=101,help='random seed')\n",
        "parser.add_argument('--save',type=str,default='./save/',help='save path')\n",
        "\n",
        "parser.add_argument('--log_print', type=str_to_bool, default=False ,help='whether to load static feature')\n",
        "\n",
        "parser.add_argument('--learning_rate',type=float,default=0.0005,help='learning rate')\n",
        "parser.add_argument('--weight_decay',type=float,default=0.0001,help='weight decay rate')\n",
        "\n",
        "parser.add_argument('--step_size1',type=int,default=400,help='step_size')\n",
        "parser.add_argument('--step_size2',type=int,default=100,help='step_size')\n",
        "\n",
        "\n",
        "#------------------#\n",
        "\n",
        "### GCT ###\n",
        "target = 'hsin_49_GCT_0600_1900_rename'\n",
        "parser.add_argument('--data_gct',type=str,default='../Data/'+target ,help='data path')\n",
        "parser.add_argument('--adj_data_gct',type=str,default='../Data/'+target+'/adj_mat_49_rename.pkl',help='adj data path')\n",
        "parser.add_argument('--num_nodes_gct',type=int,default=49,help='number of nodes/variables')\n",
        "parser.add_argument('--expid_gct',type=int,default=202306161737,help='experiment id')\n",
        "parser.add_argument('--subgraph_size_gct',type=int,default=20,help='k')\n",
        "\n",
        "\n",
        "### CCTV ###\n",
        "target = 'hsin_6_CCTV_0600_1900_rename'\n",
        "parser.add_argument('--data_cctv',type=str,default='../Data/'+target ,help='data path')\n",
        "parser.add_argument('--adj_data_cctv',type=str,default='../Data/'+target+'/adj_mat_6_rename.pkl',help='adj data path')\n",
        "parser.add_argument('--num_nodes_cctv',type=int,default=6,help='number of nodes/variables')\n",
        "parser.add_argument('--expid_cctv',type=int,default=202306161734,help='experiment id')\n",
        "parser.add_argument('--subgraph_size_cctv',type=int,default=4,help='k')\n",
        "\n",
        "\n",
        "### Fusion ###\n",
        "parser.add_argument('--expid_fusion',type=int,default=202306162115,help='experiment id')\n",
        "parser.add_argument('--runs',type=int,default=10,help='number of runs')\n",
        "parser.add_argument('--epochs',type=int,default=180,help='')\n",
        "\n",
        "#args = parser.parse_args()\n",
        "args=parser.parse_args(args=[])\n",
        "torch.set_num_threads(3)\n",
        "\n",
        "#args = parser.parse_args()\n",
        "args=parser.parse_args(args=[])\n",
        "print('# args', args)\n",
        "\n",
        "device = torch.device(args.device)\n",
        "\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-iFXSUukFaL"
      },
      "source": [
        "### Loading Data GCT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNZdTDgukFaM"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch_size = args.batch_size\n",
        "valid_batch_size = args.batch_size\n",
        "test_batch_size = args.batch_size\n",
        "data = {}\n",
        "\n",
        "_types = ''\n",
        "\n",
        "for category in ['train'+_types, 'val'+_types, 'test'+_types]:\n",
        "\n",
        "    print(\"# Loading:\", category + '.npz')\n",
        "\n",
        "    # Loading npz\n",
        "    cat_data = np.load(os.path.join(args.data_gct, category + '.npz'))\n",
        "\n",
        "    data['x_' + category] = cat_data['x']     # (?, 12, 207, 2)\n",
        "    data['y_' + category] = cat_data['y']     # (?, 12, 207, 2)\n",
        "\n",
        "    print('x[0]:',cat_data['x'][0])\n",
        "    print('y[0]:',cat_data['y'][0])\n",
        "    print('x[-1]',cat_data['x'][-1])\n",
        "    print('y[-1]',cat_data['y'][-1])\n",
        "\n",
        "# 使用train的mean/std來正規化valid/test #\n",
        "scaler = StandardScaler(mean=data['x_train'+_types][..., 0].mean(), std=data['x_train'+_types][..., 0].std())\n",
        "\n",
        "# 將欲訓練特徵改成正規化\n",
        "for category in ['train'+_types, 'val'+_types, 'test'+_types]:\n",
        "    data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\n",
        "\n",
        "\n",
        "data['train_loader'] = DataLoaderM(data['x_train'+_types], data['y_train'+_types], batch_size)\n",
        "data['val_loader'] = DataLoaderM(data['x_val'+_types], data['y_val'+_types], valid_batch_size)\n",
        "data['test_loader'] = DataLoaderM(data['x_test'+_types], data['y_test'+_types], test_batch_size)\n",
        "data['scaler'] = scaler\n",
        "\n",
        "sensor_ids, sensor_id_to_ind, adj_mx = load_adj(args.adj_data_gct,args.adjtype)   # adjtype: default='doubletransition'\n",
        "\n",
        "adj_mx_gct = [torch.tensor(i).to(device) for i in adj_mx]\n",
        "\n",
        "dataloader_gct = data.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSXbT9A4n48l"
      },
      "source": [
        "### Loading Data CCTV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnybVkKxn48m"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch_size = args.batch_size\n",
        "valid_batch_size = args.batch_size\n",
        "test_batch_size = args.batch_size\n",
        "data = {}\n",
        "\n",
        "_types = ''\n",
        "\n",
        "for category in ['train'+_types, 'val'+_types, 'test'+_types]:\n",
        "\n",
        "    print(\"# Loading:\", category + '.npz')\n",
        "\n",
        "    # Loading npz\n",
        "    cat_data = np.load(os.path.join(args.data_cctv, category + '.npz'))\n",
        "\n",
        "    data['x_' + category] = cat_data['x']     # (?, 12, 207, 2)\n",
        "    data['y_' + category] = cat_data['y']     # (?, 12, 207, 2)\n",
        "\n",
        "    print(cat_data['x'].shape)\n",
        "    print('x[0]:',cat_data['x'][0])\n",
        "    print('y[0]:',cat_data['y'][0])\n",
        "    print('x[-1]',cat_data['x'][-1])\n",
        "    print('y[-1]',cat_data['y'][-1])\n",
        "\n",
        "# 使用train的mean/std來正規化valid/test #\n",
        "scaler = StandardScaler(mean=data['x_train'+_types][..., 0].mean(), std=data['x_train'+_types][..., 0].std())\n",
        "\n",
        "# 將欲訓練特徵改成正規化\n",
        "for category in ['train'+_types, 'val'+_types, 'test'+_types]:\n",
        "    data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\n",
        "\n",
        "\n",
        "data['train_loader'] = DataLoaderM(data['x_train'+_types], data['y_train'+_types], batch_size)\n",
        "data['val_loader'] = DataLoaderM(data['x_val'+_types], data['y_val'+_types], valid_batch_size)\n",
        "data['test_loader'] = DataLoaderM(data['x_test'+_types], data['y_test'+_types], test_batch_size)\n",
        "data['scaler'] = scaler\n",
        "\n",
        "sensor_ids, sensor_id_to_ind, adj_mx = load_adj(args.adj_data_cctv,args.adjtype)   # adjtype: default='doubletransition'\n",
        "\n",
        "adj_mx_cctv = [torch.tensor(i).to(device) for i in adj_mx]\n",
        "\n",
        "dataloader_cctv = data.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vrEvY1cI8xl"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjSI-TI2I8xr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# test_model(engine,\"fusion\",dataloader_gct,checkpoint,runid,dataloader_cctv)\n",
        "def test_model(engine,model_type,dataloader,checkpoint,runid,dataloader2=None):\n",
        "\n",
        "    ### 測試讀取出的model ###\n",
        "    valid_loss = []\n",
        "    valid_mape = []\n",
        "    valid_rmse = []\n",
        "    valid_smape = []\n",
        "    s1 = time.time()\n",
        "\n",
        "    if dataloader2 != None:\n",
        "        print(\"in fusion.................\")\n",
        "        for iter, ((x1, y1), (x2, y2)) in enumerate(zip(dataloader['val_loader'].get_iterator(), dataloader2['val_loader'].get_iterator())):\n",
        "            # Process data from first loader\n",
        "            testx1 = torch.Tensor(x1).to(device)\n",
        "            testx1 = testx1.transpose(1, 3)\n",
        "            testy1 = torch.Tensor(y1).to(device)\n",
        "            testy1 = testy1.transpose(1, 3)\n",
        "\n",
        "            # Process data from second loader\n",
        "            testx2 = torch.Tensor(x2).to(device)\n",
        "            testx2 = testx2.transpose(1, 3)\n",
        "            testy2 = torch.Tensor(y2).to(device)\n",
        "            testy2 = testy2.transpose(1, 3)\n",
        "\n",
        "            metrics = engine.eval('fusion',testx1, testy1[:,0,:,:], input_2= testx2)\n",
        "\n",
        "            valid_loss.append(metrics[0])\n",
        "            valid_mape.append(metrics[1])\n",
        "            valid_rmse.append(metrics[2])\n",
        "            valid_smape.append(metrics[3])\n",
        "    else:\n",
        "        for iter, (x, y) in enumerate(dataloader['val_loader'].get_iterator()):\n",
        "            testx = torch.Tensor(x).to(device)\n",
        "            testx = testx.transpose(1, 3)\n",
        "            testy = torch.Tensor(y).to(device)\n",
        "            testy = testy.transpose(1, 3)\n",
        "            metrics = engine.eval(model_type,testx, testy[:,0,:,:])\n",
        "\n",
        "            valid_loss.append(metrics[0])\n",
        "            valid_mape.append(metrics[1])\n",
        "            valid_rmse.append(metrics[2])\n",
        "            valid_smape.append(metrics[3])\n",
        "\n",
        "    mvalid_loss = np.mean(valid_loss)\n",
        "    mvalid_mape = np.mean(valid_mape)\n",
        "    mvalid_rmse = np.mean(valid_rmse)\n",
        "    print(\"### 2-The valid loss on loding model is\", str(round(mvalid_loss,4)))\n",
        "    ### 測試讀取出的model ###\n",
        "\n",
        "\n",
        "    #valid data\n",
        "    outputs = []\n",
        "    realy = torch.Tensor(dataloader['y_val'+_types]).to(device)\n",
        "    realy = realy.transpose(1,3)[:,0,:,:]\n",
        "    print('#realy', realy.shape)\n",
        "\n",
        "    '''\n",
        "    for iter, (x, y) in enumerate(dataloader['val_loader'].get_iterator()):\n",
        "        testx = torch.Tensor(x).to(device)\n",
        "        testx = testx.transpose(1,3)\n",
        "        with torch.no_grad():\n",
        "\n",
        "            if model_type == 'gct':\n",
        "              preds = engine.model(testx)\n",
        "            elif model_type == 'cctv':\n",
        "              preds = engine.model_2(testx)\n",
        "\n",
        "            preds = preds.transpose(1,3)  # 64,1,6,12\n",
        "\n",
        "        outputs.append(preds.squeeze()) # 64,1,6,12 ->squeeze()->64,6,12\n",
        "    '''\n",
        "\n",
        "    if dataloader2 != None:\n",
        "        print(\"in fusion................. 2\")\n",
        "        for iter, ((x1, y1), (x2, y2)) in enumerate(zip(dataloader['val_loader'].get_iterator(), dataloader2['val_loader'].get_iterator())):\n",
        "            # Process data from first loader\n",
        "            testx1 = torch.Tensor(x1).to(device)\n",
        "            testx1 = testx1.transpose(1, 3)\n",
        "\n",
        "            # Process data from second loader\n",
        "            testx2 = torch.Tensor(x2).to(device)\n",
        "            testx2 = testx2.transpose(1, 3)\n",
        "\n",
        "            with torch.no_grad():  #************#\n",
        "                '''\n",
        "                preds = engine.model_3(testx1, idx=None, input_2=testx2)\n",
        "\n",
        "                preds = preds.transpose(1,3)  # 64,1,6,12\n",
        "                '''\n",
        "                # 要先經過model,model_2 encoder #\n",
        "                output = engine.model(testx1) # cctv\n",
        "                output = output.transpose(1,3)\n",
        "\n",
        "                output_2 = engine.model_2(testx2) # cctv\n",
        "                output_2 = output_2.transpose(1,3)\n",
        "\n",
        "                output = torch.cat([output,testx1[:,1].unsqueeze(1)],dim=1)\n",
        "                output_2 = torch.cat([output_2,testx2[:,1].unsqueeze(1)],dim=1)\n",
        "\n",
        "                preds = engine.model_3(output, idx=None, input_2=output_2)\n",
        "                preds = preds.transpose(1,3)  # 64,1,6,12\n",
        "\n",
        "\n",
        "            outputs.append(preds.squeeze()) # 64,1,6,12 ->squeeze()->64,6,12\n",
        "    else:\n",
        "        for iter, (x, y) in enumerate(dataloader['val_loader'].get_iterator()):\n",
        "            testx = torch.Tensor(x).to(device)\n",
        "            testx = testx.transpose(1,3)\n",
        "            with torch.no_grad():\n",
        "\n",
        "                if model_type == 'gct':\n",
        "                  preds = engine.model(testx)\n",
        "                elif model_type == 'cctv':\n",
        "                  preds = engine.model_2(testx)\n",
        "\n",
        "                preds = preds.transpose(1,3)  # 64,1,6,12\n",
        "\n",
        "            outputs.append(preds.squeeze()) # 64,1,6,12 ->squeeze()->64,6,12\n",
        "\n",
        "    yhat = torch.cat(outputs,dim=0)\n",
        "    yhat = yhat[:realy.size(0),...]  # 5240,6,12\n",
        "    print('# cat valid preds', yhat.shape)\n",
        "\n",
        "    pred = dataloader['scaler'].inverse_transform(yhat)\n",
        "\n",
        "    vmae, vmape, vrmse,vsmape = metric(pred,realy)\n",
        "    print(\"valid vmae\",vmae)\n",
        "\n",
        "    #test data\n",
        "    outputs = []\n",
        "    realy = torch.Tensor(dataloader['y_test'+_types]).to(device)\n",
        "    realy = realy.transpose(1, 3)[:, 0, :, :]\n",
        "\n",
        "\n",
        "    if dataloader2 != None:\n",
        "        for iter, ((x1, y1), (x2, y2)) in enumerate(zip(dataloader['test_loader'].get_iterator(), dataloader2['test_loader'].get_iterator())):\n",
        "            # Process data from first loader\n",
        "            testx1 = torch.Tensor(x1).to(device)\n",
        "            testx1 = testx1.transpose(1, 3)\n",
        "\n",
        "            # Process data from second loader\n",
        "            testx2 = torch.Tensor(x2).to(device)\n",
        "            testx2 = testx2.transpose(1, 3)\n",
        "            '''\n",
        "            with torch.no_grad():\n",
        "                preds = engine.model_3(testx1, idx=None, input_2=testx2)\n",
        "\n",
        "                preds = preds.transpose(1,3)  # 64,1,6,12\n",
        "            '''\n",
        "            with torch.no_grad():  #************#\n",
        "                '''\n",
        "                preds = engine.model_3(testx1, idx=None, input_2=testx2)\n",
        "\n",
        "                preds = preds.transpose(1,3)  # 64,1,6,12\n",
        "                '''\n",
        "                # 要先經過model,model_2 encoder #\n",
        "                output = engine.model(testx1) # cctv\n",
        "                output = output.transpose(1,3)\n",
        "\n",
        "                output_2 = engine.model_2(testx2) # cctv\n",
        "                output_2 = output_2.transpose(1,3)\n",
        "\n",
        "                output = torch.cat([output,testx1[:,1].unsqueeze(1)],dim=1)\n",
        "                output_2 = torch.cat([output_2,testx2[:,1].unsqueeze(1)],dim=1)\n",
        "\n",
        "                preds = engine.model_3(output, idx=None, input_2=output_2)\n",
        "                preds = preds.transpose(1,3)  # 64,1,6,12\n",
        "\n",
        "            outputs.append(preds.squeeze()) # 64,1,6,12 ->squeeze()->64,6,12\n",
        "    else:\n",
        "        for iter, (x, y) in enumerate(dataloader['test_loader'].get_iterator()):\n",
        "            testx = torch.Tensor(x).to(device)\n",
        "            testx = testx.transpose(1, 3)\n",
        "            with torch.no_grad():\n",
        "                #preds = engine.model(testx)\n",
        "                if model_type == 'gct':\n",
        "                  preds = engine.model(testx)\n",
        "                elif model_type == 'cctv':\n",
        "                  preds = engine.model_2(testx)\n",
        "\n",
        "                preds = preds.transpose(1, 3)\n",
        "            outputs.append(preds.squeeze())\n",
        "\n",
        "    yhat = torch.cat(outputs, dim=0)\n",
        "    yhat = yhat[:realy.size(0), ...]  #10478, 6, 12\n",
        "    print('# cat test preds', yhat.shape)\n",
        "\n",
        "    mae = []\n",
        "    mape = []\n",
        "    rmse = []\n",
        "    smape = []\n",
        "    for i in range(args.seq_out_len):\n",
        "\n",
        "        pred = dataloader['scaler'].inverse_transform(yhat[:, :, i])\n",
        "\n",
        "        real = realy[:, :, i]\n",
        "        metrics = metric(pred, real)\n",
        "\n",
        "        #print(metrics, 'pred', pred.shape , pred[0], 'real', real.shape, real[0])\n",
        "\n",
        "        log = 'Evaluate best model on test data for horizon {:d}, Test MAE: {:.4f}, Test MAPE: {:.4f}, Test RMSE: {:.4f}'\n",
        "        print(log.format(i + 1, metrics[0], metrics[1], metrics[2]))\n",
        "        mae.append(metrics[0])\n",
        "        mape.append(metrics[1])\n",
        "        rmse.append(metrics[2])\n",
        "        smape.append(metrics[3])\n",
        "\n",
        "    log = '{:.2f}\t{:.2f}\t{:.4f}\t{:.4f}\t'\n",
        "    print( \"##### exp\" + str(args.expid_gct) + \"_\" + str(runid)+'\t',\n",
        "          log.format(mae[0], rmse[0], smape[0], mape[0]),\n",
        "          log.format(mae[2], rmse[2], smape[2], mape[2]),\n",
        "          log.format(mae[5], rmse[5], smape[5], mape[5]),\n",
        "          log.format(mae[11], rmse[11], smape[11], mape[11]),\n",
        "         )\n",
        "\n",
        "    ### Drawing Loss Diagram ###\n",
        "    fig = plt.figure(figsize=(10, 6), dpi=100)\n",
        "    plt.plot(checkpoint['train_loss'], label=\"train loss\")\n",
        "    plt.plot(checkpoint['valid_loss'], label=\"valid loss\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.title('#Loss of Training', fontsize=20)\n",
        "    plt.ylabel(\"MAPE\", fontsize=14)\n",
        "    plt.xlabel(\"Epochs\", fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "    return vmae, vmape, vrmse,vsmape, mae, mape, rmse,smape\n",
        "\n",
        "def main(runid):\n",
        "\n",
        "\n",
        "    # if args.load_static_feature:\n",
        "    #     static_feat = load_node_feature('data/sensor_graph/location.csv')\n",
        "    # else:\n",
        "    #     static_feat = None\n",
        "\n",
        "    # num_nodes, subgraph_size, adj_mx\n",
        "    model = gginet(args.model_type, args.gcn_true, args.buildA_true, args.gcn_depth,\n",
        "                   args.num_nodes_gct, #********#\n",
        "                   device,\n",
        "                   predefined_A=adj_mx_gct,\n",
        "                   kernel_set=args.kernel_set, dropout=args.dropout,\n",
        "                   subgraph_size=args.subgraph_size_gct,  #********#\n",
        "                   node_dim=args.node_dim, dilation_exponential=args.dilation_exponential, conv_channels=args.conv_channels, residual_channels=args.residual_channels,\n",
        "                  skip_channels=args.skip_channels, end_channels= args.end_channels,\n",
        "                  seq_length=args.seq_in_len, in_dim=args.in_dim, out_dim=args.seq_out_len,\n",
        "                  layers=args.layers, propalpha=args.propalpha, tanhalpha=args.tanhalpha, layer_norm_affline=True)\n",
        "\n",
        "    model_cctv = gginet(args.model_type, args.gcn_true, args.buildA_true, args.gcn_depth,\n",
        "                   args.num_nodes_cctv, #********#\n",
        "                   device,\n",
        "                   predefined_A=adj_mx_cctv,\n",
        "                   kernel_set=args.kernel_set, dropout=args.dropout,\n",
        "                   subgraph_size=args.subgraph_size_cctv, #********#\n",
        "                   node_dim=args.node_dim, dilation_exponential=args.dilation_exponential, conv_channels=args.conv_channels, residual_channels=args.residual_channels,\n",
        "                  skip_channels=args.skip_channels, end_channels= args.end_channels,\n",
        "                  seq_length=args.seq_in_len, in_dim=args.in_dim, out_dim=args.seq_out_len,\n",
        "                  layers=args.layers, propalpha=args.propalpha, tanhalpha=args.tanhalpha, layer_norm_affline=True)\n",
        "\n",
        "    model_fusion = gginet(args.model_type, args.gcn_true, args.buildA_true, args.gcn_depth,\n",
        "                   args.num_nodes_gct, #********#\n",
        "                   device,\n",
        "                   predefined_A=adj_mx_gct,\n",
        "                   kernel_set=args.kernel_set, dropout=args.dropout,\n",
        "                   subgraph_size=args.subgraph_size_gct,  #********#\n",
        "                   node_dim=args.node_dim, dilation_exponential=args.dilation_exponential, conv_channels=args.conv_channels, residual_channels=args.residual_channels,\n",
        "                  skip_channels=args.skip_channels, end_channels= args.end_channels,\n",
        "                  seq_length=args.seq_in_len, in_dim=args.in_dim, out_dim=args.seq_out_len,\n",
        "                  layers=args.layers, propalpha=args.propalpha, tanhalpha=args.tanhalpha, layer_norm_affline=True, fusion=1)\n",
        "\n",
        "    print(model)\n",
        "    print(args)\n",
        "\n",
        "    print('The recpetive field size is', model.receptive_field)\n",
        "    nParams = sum([p.nelement() for p in model.parameters()])       # model參數量!\n",
        "    print('Number of model parameters is', nParams)\n",
        "\n",
        "    engine = Trainer(model, model_cctv,model_fusion, args.learning_rate, args.weight_decay, args.clip, args.step_size1, args.seq_out_len, dataloader_gct['scaler'], dataloader_cctv['scaler'], device, args.cl)\n",
        "\n",
        "    ###############\n",
        "    #.............#\n",
        "    ###############\n",
        "\n",
        "    # expid_gct\n",
        "    SAVE_PATH = args.save + \"exp\" + str(args.expid_gct) + \"_\" + str(runid) +\".pth\"\n",
        "    print(\"### loading model is:\",SAVE_PATH ,'###')\n",
        "    checkpoint = torch.load(SAVE_PATH)\n",
        "    engine.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    engine.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    loss = checkpoint['loss']\n",
        "    print(\"### Loading Model finished ###\")\n",
        "    print(\"### The valid loss on loding model is\", str(round(loss,4)))\n",
        "\n",
        "    # expid_cctv\n",
        "    SAVE_PATH = args.save + \"exp\" + str(args.expid_cctv) + \"_\" + str(runid) +\".pth\"\n",
        "    print(\"### loading model is:\",SAVE_PATH ,'###')\n",
        "    checkpoint = torch.load(SAVE_PATH)\n",
        "    engine.model_2.load_state_dict(checkpoint['model_state_dict'])\n",
        "    engine.optimizer_2.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    loss = checkpoint['loss']\n",
        "    print(\"### Loading Model finished ###\")\n",
        "    print(\"### The valid loss on loding model is\", str(round(loss,4)))\n",
        "\n",
        "    #-----------------------Training--------------------#\n",
        "\n",
        "    print(\"start training...\",flush=True)\n",
        "    his_loss =[]\n",
        "    val_time = []\n",
        "    train_time = []\n",
        "    minl = 1e5\n",
        "    start_epoch=0\n",
        "    SAVE_PATH = \"\"\n",
        "    train_loss_epoch = []  # 紀錄train在epoch收斂\n",
        "    valid_loss_epoch = []  # 紀錄valid在epoch收斂\n",
        "\n",
        "    for i in range(start_epoch,start_epoch+args.epochs+1):\n",
        "\n",
        "        train_loss = []\n",
        "        train_mape = []\n",
        "        train_rmse = []\n",
        "        train_smape = []\n",
        "        t1 = time.time()\n",
        "\n",
        "        for iter, ((x1, y1), (x2, y2)) in enumerate(zip(dataloader_gct['train_loader'].get_iterator(), dataloader_cctv['train_loader'].get_iterator())):\n",
        "            # Process data from first loader (GCT)\n",
        "            trainx1 = torch.Tensor(x1).to(device)\n",
        "            trainx1 = trainx1.transpose(1, 3)\n",
        "            trainy1 = torch.Tensor(y1).to(device)\n",
        "            trainy1 = trainy1.transpose(1, 3)\n",
        "\n",
        "            # Process data from second loader (CCTV)\n",
        "            trainx2 = torch.Tensor(x2).to(device)\n",
        "            trainx2 = trainx2.transpose(1, 3)\n",
        "            trainy2 = torch.Tensor(y2).to(device)\n",
        "            trainy2 = trainy2.transpose(1, 3)\n",
        "\n",
        "            # 目標仍是predict GCT\n",
        "            metrics = engine.train(trainx1,trainx2, trainy1[:,0,:,:])\n",
        "\n",
        "            train_loss.append(metrics[0])\n",
        "            train_mape.append(metrics[1])\n",
        "            train_rmse.append(metrics[2])\n",
        "            train_smape.append(metrics[3])\n",
        "\n",
        "            if iter % args.print_every == 0 :\n",
        "                log = 'Iter: {:03d}, Train Loss: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}'\n",
        "                print(log.format(iter, train_loss[-1], train_mape[-1], train_rmse[-1]),flush=True)\n",
        "\n",
        "        t2 = time.time()\n",
        "        train_time.append(t2-t1)\n",
        "        #validation\n",
        "        valid_loss = []\n",
        "        valid_mape = []\n",
        "        valid_rmse = []\n",
        "        valid_smape = []\n",
        "\n",
        "        s1 = time.time()\n",
        "\n",
        "        '''\n",
        "        for iter, (x, y) in enumerate(dataloader['val_loader'].get_iterator()):\n",
        "            testx = torch.Tensor(x).to(device)\n",
        "            testx = testx.transpose(1, 3)\n",
        "            testy = torch.Tensor(y).to(device)\n",
        "            testy = testy.transpose(1, 3)\n",
        "            metrics = engine.eval(testx, testy[:,0,:,:])\n",
        "            valid_loss.append(metrics[0])\n",
        "            valid_mape.append(metrics[1])\n",
        "            valid_rmse.append(metrics[2])\n",
        "            valid_smape.append(metrics[3])\n",
        "        '''\n",
        "\n",
        "        for iter, ((x1, y1), (x2, y2)) in enumerate(zip(dataloader_gct['val_loader'].get_iterator(), dataloader_cctv['val_loader'].get_iterator())):\n",
        "            # Process data from first loader (GCT)\n",
        "            testx1 = torch.Tensor(x1).to(device)\n",
        "            testx1 = testx1.transpose(1, 3)\n",
        "            testy1 = torch.Tensor(y1).to(device)\n",
        "            testy1 = testy1.transpose(1, 3)\n",
        "\n",
        "            # Process data from second loader (CCTV)\n",
        "            testx2 = torch.Tensor(x2).to(device)\n",
        "            testx2 = testx2.transpose(1, 3)\n",
        "            testy2 = torch.Tensor(y2).to(device)\n",
        "            testy2 = testy2.transpose(1, 3)\n",
        "\n",
        "            metrics = engine.eval('fusion',testx1, testy1[:,0,:,:], input_2= testx2)\n",
        "\n",
        "            valid_loss.append(metrics[0])\n",
        "            valid_mape.append(metrics[1])\n",
        "            valid_rmse.append(metrics[2])\n",
        "            valid_smape.append(metrics[3])\n",
        "\n",
        "        s2 = time.time()\n",
        "        log = 'Epoch: {:03d}, Inference Time: {:.4f} secs'\n",
        "        print(log.format(i,(s2-s1)))\n",
        "        val_time.append(s2-s1)\n",
        "        mtrain_loss = np.mean(train_loss)\n",
        "        mtrain_mape = np.mean(train_mape)\n",
        "        mtrain_rmse = np.mean(train_rmse)\n",
        "        mtrain_smape = np.mean(train_smape)\n",
        "\n",
        "        mvalid_loss = np.mean(valid_loss)\n",
        "        mvalid_mape = np.mean(valid_mape)\n",
        "        mvalid_rmse = np.mean(valid_rmse)\n",
        "        mvalid_smape = np.mean(valid_smape)\n",
        "        his_loss.append(mvalid_smape)\n",
        "\n",
        "        log = 'Epoch: {:03d}, Train Loss: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}, Valid Loss: {:.4f}, Valid MAPE: {:.4f}, Valid RMSE: {:.4f}, Training Time: {:.4f}/epoch'\n",
        "        print(log.format(i, mtrain_loss, mtrain_mape, mtrain_rmse, mvalid_loss, mvalid_mape, mvalid_rmse, (t2 - t1)),flush=True)\n",
        "        # 紀錄每個epoch的loss\n",
        "        train_loss_epoch.append(mtrain_loss)\n",
        "        valid_loss_epoch.append(mvalid_loss)\n",
        "\n",
        "        if mvalid_loss<minl:\n",
        "            #torch.save(engine.model.state_dict(), args.save + \"exp\" + str(args.expid) + \"_\" + str(runid) +\".pth\")\n",
        "            SAVE_PATH = args.save + \"exp\" + str(args.expid_fusion) + \"_\" + str(runid) +\".pth\"\n",
        "            print(\"### Update Best Model:\",SAVE_PATH, \" ###\")\n",
        "            torch.save({\n",
        "              'epoch': i,\n",
        "              'task_level': engine.task_level,\n",
        "              'model_state_dict': engine.model_3.state_dict(),   #*******#\n",
        "              'optimizer_state_dict': engine.optimizer_3.state_dict(), #*******#\n",
        "              'loss': mvalid_loss,\n",
        "              'train_loss': train_loss_epoch,\n",
        "              'valid_loss': valid_loss_epoch\n",
        "            }, SAVE_PATH)\n",
        "            minl = mvalid_loss\n",
        "\n",
        "    print(\"Average Training Time: {:.4f} secs/epoch\".format(np.mean(train_time)))\n",
        "    print(\"Average Inference Time: {:.4f} secs\".format(np.mean(val_time)))\n",
        "\n",
        "\n",
        "    bestid = np.argmin(his_loss)\n",
        "\n",
        "    writer.close()\n",
        "    print(\"Training finished\")\n",
        "    print(\"The valid loss on best model is\", str(round(his_loss[bestid],4)))\n",
        "\n",
        "    #-----------------------Training-------------------#\n",
        "\n",
        "    print(\"--------------------CCTV------------------\")\n",
        "    vmae, vmape, vrmse,vsmape, mae, mape, rmse,smape = test_model(engine,\"cctv\",dataloader_cctv,checkpoint,runid)\n",
        "    print(\"--------------------GCT------------------\")\n",
        "    vmae, vmape, vrmse,vsmape, mae, mape, rmse,smape = test_model(engine,\"gct\",dataloader_gct,checkpoint,runid)\n",
        "    print(\"--------------------Fusion------------------\")\n",
        "    # expid_cctv\n",
        "    SAVE_PATH = args.save + \"exp\" + str(args.expid_fusion) + \"_\" + str(runid) +\".pth\"\n",
        "    print(\"### loading model is:\",SAVE_PATH ,'###')\n",
        "    checkpoint = torch.load(SAVE_PATH)\n",
        "    engine.model_3.load_state_dict(checkpoint['model_state_dict'])   #*******#\n",
        "    engine.optimizer_3.load_state_dict(checkpoint['optimizer_state_dict']) #*******#\n",
        "    loss = checkpoint['loss']\n",
        "    print(\"### Loading Model finished ###\")\n",
        "    print(\"### The valid loss on loding model is\", str(round(loss,4)))\n",
        "    vmae, vmape, vrmse,vsmape, mae, mape, rmse,smape = test_model(engine,\"fusion\",dataloader_gct,checkpoint,runid,dataloader_cctv)\n",
        "\n",
        "    #sys.exit()\n",
        "\n",
        "    return vmae, vmape, vrmse,vsmape, mae, mape, rmse,smape\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    vmae = []\n",
        "    vmape = []\n",
        "    vrmse = []\n",
        "    vsmape = []\n",
        "    mae = []\n",
        "    mape = []\n",
        "    rmse = []\n",
        "    smape = []\n",
        "    for i in range(args.runs):\n",
        "        vm1, vm2, vm3,vm4, m1, m2, m3, m4 = main(i)\n",
        "        vmae.append(vm1)\n",
        "        vmape.append(vm2)\n",
        "        vrmse.append(vm3)\n",
        "        vsmape.append(vm4)\n",
        "        mae.append(m1)\n",
        "        mape.append(m2)\n",
        "        rmse.append(m3)\n",
        "        smape.append(m4)\n",
        "\n",
        "    mae = np.array(mae)\n",
        "    mape = np.array(mape)\n",
        "    rmse = np.array(rmse)\n",
        "    smape = np.array(smape)\n",
        "\n",
        "    amae = np.mean(mae,0)\n",
        "    amape = np.mean(mape,0)\n",
        "    armse = np.mean(rmse,0)\n",
        "    asmape = np.mean(smape,0)\n",
        "\n",
        "    smae = np.std(mae,0)\n",
        "    s_mape = np.std(mape,0)\n",
        "    srmse = np.std(rmse,0)\n",
        "    s_smape = np.std(smape,0)\n",
        "\n",
        "    print('\\n\\nResults for 10 runs\\n\\n')\n",
        "    #valid data\n",
        "    print('valid\\tMAE\\tRMSE\\tMAPE')\n",
        "    log = 'mean:\\t{:.4f}\\t{:.4f}\\t{:.4f}'\n",
        "    print(log.format(np.mean(vmae),np.mean(vrmse),np.mean(vmape)))\n",
        "    log = 'std:\\t{:.4f}\\t{:.4f}\\t{:.4f}'\n",
        "    print(log.format(np.std(vmae),np.std(vrmse),np.std(vmape)))\n",
        "    print('\\n\\n')\n",
        "    #test data\n",
        "    print('test|horizon\\tMAE-mean\\tRMSE-mean\\tMAPE-mean\\tMAE-std\\tRMSE-std\\tMAPE-std')\n",
        "    for i in [2,5,11]:\n",
        "        log = '{:d}\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}'\n",
        "        print(log.format(i+1, amae[i], armse[i], amape[i], smae[i], srmse[i], s_mape[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新增區段"
      ],
      "metadata": {
        "id": "aYjBDCNs-Eyq"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}